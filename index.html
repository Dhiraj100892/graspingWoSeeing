<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>Grasping Without Seeing</title>

    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/github-light.css">
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    </head>
  <body>
    <div class="wrapper">
      <header>
        <h1 class="header"><a href="https://dhiraj100892.github.io/graspingWoSeeing/">Grasping Without Seeing</a></h1>
        <p class="header"></p>

        <h2 class="header"><a href="https://dhiraj100892.github.io/graspingWoSeeing/">arXiv</a></h2>
        <p class="header"></p>

       	<h2 class="header"><a href="#data">Data</a></h2>
        <p class="header"></p>

        <h2 class="header"><a href="#contact">Contact</a></h2>
        <p class="header"></p>

      </header>

      <section>
      <html>
      <body>
	<iframe width="620" height="340" src="https://www.youtube.com/embed/iCQsM7EE4HI" frameborder="0" allowfullscreen></iframe>
      </body>
      </html>
      <p>
      <br/>
      <br/>

<h1>Overview</h1>

How can a robot grasp an unknown object without seeing it? In this paper, we present an attempt for grasping novel objects using tactile sensing and without prior knowledge of the objects' location or physical properties. Our key idea is to combine touch based object localization with tactile based re-grasping. To train our learning models, we created a large-scale grasping dataset with haptic recordings, images and material labels. For haptic representation, we propose an unsupervised auto-encoding scheme for learning haptic features, which shows a significant improvement over prior methods on a variety of tactile perception tasks. First, our touch localization model sequentially ``touch-scans'' the workspace and uses a particle filter to aggregate beliefs from multiple hits of the target. It outputs an estimate of the object's location, from which an initial grasp is established. Next, our re-grasping model learns to progressively improve grasps with haptic feedback based on the learnt features. This network learns to estimate grasp stability and predict adjustment for next grasp. Re-grasping thus is performed iteratively until our model identifies a stable grasp. Finally, we demonstrate extensive experimental results on grasping a novel set of objects using a real world manipulator. 

<h1 id="data">Data</h1>

Here you can find documentation regarding the dataset collected. It contains haptic, visual and kinematic data on about 7800 grasp interactions on diverse objects collected with a multi-fingered underactuated adaptive gripper. The full dataset can be downloaded <a href="http://adithyamurali.com/">here</a>. A visualization of a sample of the data and more detailed information is given below.
</p>

            <img src="data.png" width=620 height=500>

<p>
There are two main subsets of the data:
<ul>
<li> 52 diverse objects with labels (material, object) with 50-60 trials per object. Each trial has exactly 2 grasps (initial grasp + re-grasp)</li>
<li>A miscellaneous unlabelled set of objects, where each trial has an arbitrary number of grasps (initial grasp + 2-3 re-grasps). </li>
</ul>
Detailed information on how the data is structured internally can be found in the <a href="http://adithyamurali.com/">README</a>.
</p>


<h1 id="paper">Paper</h1>
The paper can be downloaded from <a href="http://adithyamurali.com/"> arXiv</a>.

      <h1 id="contact"><font color="black">Contributors</font></h1>

      <a href="http://adithyamurali.com/">Adithya Murali</a>, <a href="http://yinli.cvpr.net/">Yin Li</a>, Dhiraj Gandhi, <a href="http://www.cs.cmu.edu/~abhinavg/">Abhinav Gupta</a></p>

      <b>Contact</b>: amurali AT andrew DOT cmu DOT edu

      </section>
    </div>
		
  </body>
</html>

